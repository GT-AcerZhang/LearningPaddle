# 比赛简介

> [2019广东工业智造创新大赛](https://tianchi.aliyun.com/competition/entrance/231748/introduction)
>
> **赛场一：布匹疵点智能识别**
>
> # 大赛概况
>
> 人工智能是国家战略性新兴产业。随着广东制造产业信息建设的不断完善，且产业布局较为完整，诞生了一批信息化程度高的工业制造企业，已沉淀积累了一定数据资源。2019年广东省人民政府联合阿里巴巴集团共同启动“广东工业智造创新大赛”，聚焦布匹疵点智能识别和面料剪裁利用率优化，旨在通过数据开放召集全球众智，将重点围绕工业制造大数据展开，以落地为导向，聚集全球顶级人才，发掘全球先进的智能制造应用成果，推动人工智能技术在广东纺织行业的探索与发展，用技术驱动广东智能制造产业转型升级和变革发展。
>
> 布匹疵点检验是纺织行业生产和质量管理的重要环节，而布匹疵点智能检测是困扰行业多年的技术瓶颈。目前几乎都是人工检测，易受主观因素影响，缺乏一致性；并且检测人员在强光下长时间工作对视力影响极大。借助人工智能和计算机视觉等先进技术，实现布匹疵点智能检测，其价值无疑是巨大的。
>
> 本赛场聚焦布匹疵点智能检测，要求选手研究开发高效可靠的计算机视觉算法，提升布匹疵点检验的准确度，降低对大量人工的依赖，提升布样疵点质检的效果和效率。初赛阶段考察素色布瑕疵检测和分类能力，复赛阶段考察花色布的瑕疵检测和分类能力。
>
> # 竞赛题目
>
> 在布匹的实际生产过程中，由于各方面因素的影响，会产生污渍、破洞、毛粒等瑕疵，为保证产品质量，需要对布匹进行瑕疵检测。布匹疵点检验是纺织行业生产和质量管理的重要环节，目前人工检测易受主观因素影响，缺乏一致性；并且检测人员在强光下长时间工作对视力影响极大。由于布匹疵点种类繁多、形态变化多样、观察识别难道大，导致布匹疵点智能检测是困扰行业多年的技术瓶颈。
> ​
> 近年来，人工智能和计算机视觉等技术突飞猛进，在工业质检场景中也取得了不错的成果。纺织行业迫切希望借助最先进的技术，实现布匹疵点智能检测。革新质检流程，自动完成质检任务，降低对大量人工的依赖，减少漏检发生率，提高产品的质量。
>
> 本赛场聚焦布匹疵点智能检测，要求选手研究开发高效可靠的计算机视觉算法，提升布匹疵点检验的准确度，降低对大量人工的依赖，提升布样疵点质检的效果和效率。要求算法既要检测布匹是否包含疵点，又要给出疵点具体的位置和类别，既考察疵点检出能力、也考察疵点定位和分类能力。
>
> # 竞赛数据
>
> 赛题组深入佛山南海纺织车间现场采集布匹图像，制作并发布大规模的高质量布匹疵点数据集，同时提供精细的标注来满足算法要求。大赛数据涵盖了纺织业中布匹的各类重要瑕疵，每张图片含一个或多种瑕疵。数据包括包括素色布和花色布两类，其中，素色布数据约8000张，用于初赛；花色布数据约12000张，用于复赛。
>
> ## 初赛数据示例
>
> 纯色布数据包含无疵点图片、有疵点图片和瑕疵的标注数据。标注数据详细标注出疵点所在的具体位置和疵点类别，数据示例如下。
> ![enter image description here](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/156519676838080781565196767650.jpeg)
>
> #### 数据提供
>
> 1.初赛训练集分三部分，分别于8.8日、8.18日、8.28日提供下载,共计约6000张，包含所有瑕疵的类型。
> 2.初赛一阶段测试集于8.18日提供下载，初赛二阶段测试集于9.19日提供下载。
>
> ## 复赛数据示例
>
> 花色布数据包含原始图片、模板图片和瑕疵的标注数据。标注数据详细标注出疵点所在的具体位置和疵点类别，，数据示例如下。
> ![enter image description here](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/156519719116060451565197190838.jpeg)
>
> #### 数据提供
>
> 1.复赛训练集分两部分，分别于9.24日、10.4日提供下载,共计约8000张，包含所有瑕疵的类型。
> 2.复赛一阶段测试集于9.24日提供下载，复赛二阶段测试集于10.23日提供下载。
>
> ## 训练数据文件结构
>
> a) 我们将提供用于训练的图像数据和识别标签，文件夹结构：
> o defect Images
> ​ o normal Images
> ​ o Annotations
> ​ o [README.md](http://readme.md/)
> b) defect Images : 存放有瑕疵的图像数据，normal Images存放无疵点的图像数据，jpeg编码图像文件。
> c) Annotations : 存放属性标签标注数据。
> d) [README.md](http://readme.md/)：对数据的详细介绍。
>
> # 提交说明
>
> ## 评测结果提交
>
> 参赛者需要提交一份json文件，文件内容如下：
>
> ```
> [
>     {
>         "name": "1560926838524.jpg",
>         "category": 1,
>         "bbox": [339.66,15.65,356.00,61.33],
>         "score": 0.0087397042
>     },
>     {
>         "name": "1560926838524.jpg",
>         "category": 2,
>         "bbox": [52.86,174.86,104.91,633.56],
>         "score": 0.5624360123
>     },
>     ... ...
>     ,
>     {
>         "name": "257092683862.jpg",
>         "category": 2,
>         "bbox": [339.61,7.21,355.94,48.61],
>         "score": 0.0135210491
>     }
> ]
> ```
>
> #### 格式说明
>
> 1.提交的json文件中包含多个疵点样本，每个疵点样本都包含name、category、bbox、score四个字段。
> 2.name字段为图片名称；category字段为类别标签；bbox为xyxy格式坐标框，小数点后保留2位；score为置信度概率，范围0-1；
> 3.对于单张图片存在多个疵点样本时，依次列出即可；对于不存在疵点的无疵点图片，不能出现在json列表中。
> 4.name字段不允许出现非测试集中的图片名。
>
> #### 生成示例
>
> json文件代码生成存储示例如下（python）：
>
> ```
> result=[]
> result.append({'name': image_name(str),'category': defect_label(int),'bbox':bbox(xyxy,float),'score': score(float)})
> import json
> with open('result.json', 'w') as fp:
>      json.dump(result, fp, indent=4, separators=(',', ': '))
> ```

# 比赛整体分析

个人认为这个比赛很有实际意义，如果对微小的布匹瑕疵点都可以比较准确定位的话，目标检测算法将具备真正在制造业推广的价值。

最终提交的成绩在初赛排名177（第二次新增的测试集没有及时加入，不过对成绩应该没有影响，第二次测试集更多是正样本）

算法方面，基本是直接拿开源框架来调参，没有新增层或参数。个人觉得虽然这样决定了成绩不会太好看，但是可能实用性会更强一些，如果过于匹配该比赛的数据集，模型要迁移到类似制丝识别杂物的场景可能改造量会很大；就当前的情况看，也没有足够的时间深入学习思考具体的改造方法。

另外看了阿里巴巴在工业场景的[解决方案](https://www.aliyun.com/solution/industry/visualcognition?spm=5176.13342246.h2v3icoap.589.5dcb3ccbIlbGoR&aly_as=fyMsJidz)，用的其实也是通用模型

![file](https://img.alicdn.com/tfs/TB1x4agbgFY.1VjSZFqXXadbXXa-1410-680.png)

# 数据集格式转换

目标检测类的标准数据集基本就是coco和pascal voc，不过似乎看起来目前以coco数据集更多，因此需要参考官方提供的转化代码 [Fabric2COCO](https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.43b46448MJ99CB&postId=71169)

```python
'''
@javis
'''
import os
import json
import numpy as np
import shutil
import pandas as pd


defect_name2label = {
    '破洞': 1, '水渍': 2, '油渍': 2, '污渍': 2, '三丝': 3, '结头': 4, '花板跳': 5, '百脚': 6, '毛粒': 7,
    '粗经': 8, '松经': 9, '断经': 10, '吊经': 11, '粗维': 12, '纬缩': 13, '浆斑': 14, '整经结': 15, '星跳': 16, '跳花': 16,
    '断氨纶': 17, '稀密档': 18, '浪纹档': 18, '色差档': 18, '磨痕': 19, '轧痕': 19, '修痕': 19, '烧毛痕': 19, '死皱': 20, '云织': 20,
    '双纬': 20, '双经': 20, '跳纱': 20, '筘路': 20, '纬纱不良': 20,
}

class Fabric2COCO:

    def __init__(self, mode="train"):
        self.images = []
        self.annotations = []
        self.categories = []
        self.img_id = 0
        self.ann_id = 0
        self.mode = mode
        if not os.path.exists("data/coco/images/{}".format(self.mode)):
            os.makedirs("data/coco/images/{}".format(self.mode))

    def to_coco(self, anno_file, img_dir):
        self._init_categories()
        anno_result = pd.read_json(open(anno_file, "r"))
        # anno_result = anno_result.sample(frac=1, random_state=2019)
        if self.mode == "train":
            anno_result = anno_result.head(int(anno_result['name'].count()*0.9))
        else:
            anno_result = anno_result.tail(int(anno_result['name'].count()*0.1)+1)
        name_list=anno_result["name"].unique()
        for img_name in name_list:
            img_anno = anno_result[anno_result["name"] == img_name]
            bboxs = img_anno["bbox"].tolist()
            defect_names = img_anno["defect_name"].tolist()
            assert img_anno["name"].unique()[0] == img_name

            img_path=os.path.join(img_dir,img_name)
            # img =cv2.imread(img_path)
            # h,w,c=img.shape
            h, w = 1000, 2446
            self.images.append(self._image(img_path, h, w))

            self._cp_img(img_path)

            for bbox, defect_name in zip(bboxs, defect_names):
                label = defect_name2label[defect_name]
                annotation = self._annotation(label, bbox)
                self.annotations.append(annotation)
                self.ann_id += 1
            self.img_id += 1
        instance = {}
        instance['info'] = 'fabric defect'
        instance['license'] = ['none']
        instance['images'] = self.images
        instance['annotations'] = self.annotations
        instance['categories'] = self.categories
        return instance

    def _init_categories(self):
        for v in range(1,21):
            print(v)
            category = {}
            category['id'] = v
            category['name'] = str(v)
            category['supercategory'] = 'defect_name'
            self.categories.append(category)
        # for k, v in defect_name2label.items():
        #     category = {}
        #     category['id'] = v
        #     category['name'] = k
        #     category['supercategory'] = 'defect_name'
        #     self.categories.append(category)

    def _image(self, path,h,w):
        image = {}
        image['height'] = h
        image['width'] = w
        image['id'] = self.img_id
        image['file_name'] = os.path.basename(path)
        return image

    def _annotation(self, label, bbox):
        area = (bbox[2]-bbox[0])*(bbox[3]-bbox[1])
        points = [[bbox[0],bbox[1]],[bbox[2],bbox[1]],[bbox[2],bbox[3]],[bbox[0],bbox[3]]]
        annotation = {}
        annotation['id'] = self.ann_id
        annotation['image_id'] = self.img_id
        annotation['category_id'] = label
        annotation['segmentation'] = [np.asarray(points).flatten().tolist()]
        annotation['bbox'] = self._get_box(points)
        annotation['iscrowd'] = 0
        annotation['area'] = area
        return annotation

    def _cp_img(self, img_path):
        shutil.copy(img_path, os.path.join("data/coco/images/{}".format(self.mode), os.path.basename(img_path)))

    def _get_box(self, points):
        min_x = min_y = np.inf
        max_x = max_y = 0
        for x, y in points:
            min_x = min(min_x, x)
            min_y = min(min_y, y)
            max_x = max(max_x, x)
            max_y = max(max_y, y)
        '''coco,[x,y,w,h]'''
        return [min_x, min_y, max_x - min_x, max_y - min_y]
    def save_coco_json(self, instance, save_path):
        import json
        with open(save_path, 'w') as fp:
            json.dump(instance, fp, indent=1, separators=(',', ': '))

if __name__ == '__main__':

    '''转换有瑕疵的样本为coco格式'''
    img_dir = "../guangdong1_round1_train2_20190828/defect_Images"
    anno_dir = "../guangdong1_round1_train2_20190828/Annotations/anno_train.json"
    fabric2coco = Fabric2COCO()
    train_instance = fabric2coco.to_coco(anno_dir, img_dir)
    if not os.path.exists("data/coco/annotations/"):
        os.makedirs("data/coco/annotations/")
    fabric2coco.save_coco_json(train_instance, "data/coco/annotations/"+'instances_{}.json'.format("train"))
```

# 基于mmdetection的模型

> 日期:2019-09-16 17:32:42
> score: 59.6398
> acc: 87.6000
> mAP: 52.6498

该结果也是最后提交成绩

基本是[布匹疵点智能识别方案介绍及baseline(acc:85%左右,mAP:52%左右)](https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.6.43b46448ifZgco&postId=74264)的复现

首先[mmdetection](https://github.com/open-mmlab/mmdetection)是商汤科技和港科大开源的基于pytorch的目标 检测领域目前比较流行的开发框架，个人猜测大部分排名前列的算法都基于该框架进行了改造

更重要的是[mmdetection](https://github.com/open-mmlab/mmdetection)提供了coco数据集预训练权重[model_zoo](https://github.com/open-mmlab/mmdetection/blob/master/docs/MODEL_ZOO.md)，并且在上述方案中，可以很好地迁移这些预训练权重。

## 改造配置文件

mmdetection的配置文件都在config目录下，这里改的是cascade_rcnn_r50_fpn_1x.py

```python
# model settings
model = dict(
    type='CascadeRCNN',
    num_stages=3,
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        style='pytorch',
        dcn=dict(   #在最后一个阶段加入可变形卷积 改进点1
            modulated=False, deformable_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, False, True)),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_scales=[8],
        anchor_ratios=[0.02, 0.05, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0], #根据样本瑕疵尺寸分布，修改anchor的长宽比。 改进点2
        anchor_strides=[4, 8, 16, 32, 64],
        target_means=[.0, .0, .0, .0],
        target_stds=[1.0, 1.0, 1.0, 1.0],
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),   #此处可替换成focalloss
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),
    bbox_roi_extractor=dict(
        type='SingleRoIExtractor',
        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    bbox_head=[
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=21,  #类别数+1(背景类)
            target_means=[0., 0., 0., 0.],
            target_stds=[0.1, 0.1, 0.2, 0.2],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=21,  #类别数+1(背景类)
            target_means=[0., 0., 0., 0.],
            target_stds=[0.05, 0.05, 0.1, 0.1],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=21,  #类别数+1(背景类)
            target_means=[0., 0., 0., 0.],
            target_stds=[0.033, 0.033, 0.067, 0.067],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
    ])
# model training and testing settings
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                ignore_iof_thr=-1),
            sampler=dict(   #默认使用的是随机采样RandomSampler，这里替换成OHEM采样，即每个级联层引入在线难样本学习，改进点3
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.7,
                min_pos_iou=0.7,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ],
    stage_loss_weights=[1, 0.5, 0.25])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),
    keep_all_stages=False)
# dataset settings
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1223, 500), keep_ratio=True), #考虑算力原有限，修改图像尺寸为半图，可修改为全图训练
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1223, 500),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    imgs_per_gpu=2,  #每张gpu训练多少张图片  batch_size = gpu_num(训练使用gpu数量) * imgs_per_gpu
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_train2017.json', #修改成自己的训练集标注文件路径
        img_prefix=data_root + 'train2017/', #训练图片路径
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json', #修改成自己的验证集标注文件路径
        img_prefix=data_root + 'val2017/', #验证图片路径
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',#修改成自己的验证集标注文件路径
        img_prefix=data_root + 'val2017/', #测试图片路径
        pipeline=test_pipeline))

# optimizer
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)  #学习率的设置尤为关键：lr = 0.00125*batch_size
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1.0 / 3,
    step=[8, 11])
checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
# runtime settings
total_epochs = 12
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/cascade_rcnn_r50_fpn_1x' #训练的权重和日志保存路径
load_from = './cascade_rcnn_r50_coco_pretrained_weights_classes_21.pth' #采用coco预训练模型 ,需要对权重类别数进行处理
resume_from = None
workflow = [('train', 1)]
```

## 下载预训练权重

```bash
wget https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/cascade_rcnn_r50_fpn_1x_20190501-3b6211ab.pth
```

## coco预训练权重类别数转换

个人觉得这个部分非常关键，也是该方案特别高效的原因。同样的模型结构，在paddlepaddle上从头训练需要3天左右（配置还比个人单机强数倍），而且似乎效果也不太好。通过类别数转换，可以直接在mmdetection的预训练权重上finetune，单机只需要训练不到1天时间。

修改执行训练的tool/train.py

```python
from __future__ import division
import argparse
import os

import torch
from mmcv import Config

from mmdet import __version__
from mmdet.apis import (get_root_logger, init_dist, set_random_seed,
                        train_detector)
from mmdet.datasets import build_dataset
from mmdet.models import build_detector

import ssl
ssl._create_default_https_context = ssl._create_unverified_context

def parse_args():
    parser = argparse.ArgumentParser(description='Train a detector')
    parser.add_argument('config', help='train config file path')
    parser.add_argument('--work_dir', help='the dir to save logs and models')
    parser.add_argument(
        '--resume_from', help='the checkpoint file to resume from')
    parser.add_argument(
        '--validate',
        action='store_true',
        help='whether to evaluate the checkpoint during training')
    parser.add_argument(
        '--gpus',
        type=int,
        default=1,
        help='number of gpus to use '
        '(only applicable to non-distributed training)')
    parser.add_argument('--seed', type=int, default=None, help='random seed')
    parser.add_argument(
        '--launcher',
        choices=['none', 'pytorch', 'slurm', 'mpi'],
        default='none',
        help='job launcher')
    parser.add_argument('--local_rank', type=int, default=0)
    parser.add_argument(
        '--autoscale-lr',
        action='store_true',
        help='automatically scale lr with the number of gpus')
    args = parser.parse_args()
    if 'LOCAL_RANK' not in os.environ:
        os.environ['LOCAL_RANK'] = str(args.local_rank)

    return args

# def main():
#     #gen coco pretrained weight
#     import torch
#     num_classes = 21
#     model_coco = torch.load("cascade_rcnn_r50_fpn_1x_20190501-3b6211ab.pth")
#
#     # weight
#     model_coco["state_dict"]["bbox_head.0.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.0.fc_cls.weight"][
#                                                             :num_classes, :]
#     model_coco["state_dict"]["bbox_head.1.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.1.fc_cls.weight"][
#                                                             :num_classes, :]
#     model_coco["state_dict"]["bbox_head.2.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.2.fc_cls.weight"][
#                                                             :num_classes, :]
#     # bias
#     model_coco["state_dict"]["bbox_head.0.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.0.fc_cls.bias"][
#                                                           :num_classes]
#     model_coco["state_dict"]["bbox_head.1.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.1.fc_cls.bias"][
#                                                           :num_classes]
#     model_coco["state_dict"]["bbox_head.2.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.2.fc_cls.bias"][
#                                                           :num_classes]
#     # save new model
#     torch.save(model_coco, "cascade_rcnn_r50_coco_pretrained_weights_classes_%d.pth" % num_classes)
#
# if __name__ == "__main__":
#     main()


def main():
    #gen coco pretrained weight
    import torch
    num_classes = 21
    model_coco = torch.load("cascade_rcnn_r50_fpn_1x_20190501-3b6211ab.pth")

    # weight
    model_coco["state_dict"]["bbox_head.0.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.0.fc_cls.weight"][
                                                            :num_classes, :]
    model_coco["state_dict"]["bbox_head.1.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.1.fc_cls.weight"][
                                                            :num_classes, :]
    model_coco["state_dict"]["bbox_head.2.fc_cls.weight"] = model_coco["state_dict"]["bbox_head.2.fc_cls.weight"][
                                                            :num_classes, :]
    # bias
    model_coco["state_dict"]["bbox_head.0.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.0.fc_cls.bias"][
                                                          :num_classes]
    model_coco["state_dict"]["bbox_head.1.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.1.fc_cls.bias"][
                                                          :num_classes]
    model_coco["state_dict"]["bbox_head.2.fc_cls.bias"] = model_coco["state_dict"]["bbox_head.2.fc_cls.bias"][
                                                          :num_classes]
    # save new model
    torch.save(model_coco, "cascade_rcnn_r50_coco_pretrained_weights_classes_%d.pth" % num_classes)
    args = parse_args()

    cfg = Config.fromfile(args.config)
    # set cudnn_benchmark
    if cfg.get('cudnn_benchmark', False):
        torch.backends.cudnn.benchmark = True
    # update configs according to CLI args
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    if args.resume_from is not None:
        cfg.resume_from = args.resume_from
    cfg.gpus = args.gpus

    if args.autoscale_lr:
        # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)
        cfg.optimizer['lr'] = cfg.optimizer['lr'] * cfg.gpus / 8

    # init distributed env first, since logger depends on the dist info.
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)

    # init logger before other steps
    logger = get_root_logger(cfg.log_level)
    logger.info('Distributed training: {}'.format(distributed))

    # set random seeds
    if args.seed is not None:
        logger.info('Set random seed to {}'.format(args.seed))
        set_random_seed(args.seed)

    model = build_detector(
        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)

    datasets = [build_dataset(cfg.data.train)]
    if len(cfg.workflow) == 2:
        datasets.append(build_dataset(cfg.data.val))
    if cfg.checkpoint_config is not None:
        # save mmdet version, config file content and class names in
        # checkpoints as meta data
        cfg.checkpoint_config.meta = dict(
            mmdet_version=__version__,
            config=cfg.text,
            CLASSES=datasets[0].CLASSES)
    # add an attribute for visualization convenience
    model.CLASSES = datasets[0].CLASSES
    train_detector(
        model,
        datasets,
        cfg,
        distributed=distributed,
        validate=args.validate,
        logger=logger)

if __name__ == '__main__':
    main()
```

## 训练并生成结果

单卡训练

```
python tools/train.py configs/cascade_rcnn_r50_fpn_1x.py --gpus 1 --validate
```

- gpus表示训练使用gpu的数量
- validate表示在训练过程中是否进行评估

双卡分布式训练

```
sh tools/dist_train.sh configs/cascade_rcnn_r50_fpn_1x.py 2
```

生成提交结果

```python
import time, os
import json
import mmcv
from mmdet.apis import init_detector, inference_detector


def main():
    config_file = 'configs/cascade_rcnn_r50_fpn_1x.py'  # 修改成自己的配置文件
    checkpoint_file = 'work_dirs/cascade_rcnn_r50_fpn_1x/epoch_12.pth'  # 修改成自己的训练权重

    test_path = 'data/coco/test'  # 官方测试集图片路径

    json_name = "result_" + "" + time.strftime("%Y%m%d%H%M%S", time.localtime()) + ".json"

    model = init_detector(config_file, checkpoint_file, device='cuda:0')

    img_list = []
    for img_name in os.listdir(test_path):
        if img_name.endswith('.jpg'):
            img_list.append(img_name)

    result = []
    for i, img_name in enumerate(img_list, 1):
        full_img = os.path.join(test_path, img_name)
        predict = inference_detector(model, full_img)
        for i, bboxes in enumerate(predict, 1):
            if len(bboxes) > 0:
                defect_label = i
                print(i)
                image_name = img_name
                for bbox in bboxes:
                    x1, y1, x2, y2, score = bbox.tolist()
                    x1, y1, x2, y2 = round(x1, 2), round(y1, 2), round(x2, 2), round(y2, 2)  # save 0.00
                    result.append(
                        {'name': image_name, 'category': defect_label, 'bbox': [x1, y1, x2, y2], 'score': score})

    with open(json_name, 'w') as fp:
        json.dump(result, fp, indent=4, separators=(',', ': '))


if __name__ == "__main__":
    main()
```

不过我在训练时遇到了一些问题，似乎要修改pipeline的源码`mmdet/datasets/pipelines/formating.py`

```python
from collections.abc import Sequence

import mmcv
import numpy as np
import torch
from mmcv.parallel import DataContainer as DC

from ..registry import PIPELINES


def to_tensor(data):
    """Convert objects of various python types to :obj:`torch.Tensor`.

    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,
    :class:`Sequence`, :class:`int` and :class:`float`.
    """
    if isinstance(data, torch.Tensor):
        return data
    elif isinstance(data, np.ndarray):
        return torch.from_numpy(data)
    elif isinstance(data, Sequence) and not mmcv.is_str(data):
        return torch.tensor(data)
    elif isinstance(data, int):
        return torch.LongTensor([data])
    elif isinstance(data, float):
        return torch.FloatTensor([data])
    else:
        raise TypeError('type {} cannot be converted to tensor.'.format(
            type(data)))


@PIPELINES.register_module
class ToTensor(object):

    def __init__(self, keys):
        self.keys = keys

    def __call__(self, results):
        for key in self.keys:
            results[key] = to_tensor(results[key])
        return results

    def __repr__(self):
        return self.__class__.__name__ + '(keys={})'.format(self.keys)


@PIPELINES.register_module
class ImageToTensor(object):

    def __init__(self, keys):
        self.keys = keys

    def __call__(self, results):
        for key in self.keys:
            results[key] = to_tensor(results[key].transpose(2, 0, 1))
        return results

    def __repr__(self):
        return self.__class__.__name__ + '(keys={})'.format(self.keys)


@PIPELINES.register_module
class Transpose(object):

    def __init__(self, keys, order):
        self.keys = keys
        self.order = order

    def __call__(self, results):
        for key in self.keys:
            results[key] = results[key].transpose(self.order)
        return results

    def __repr__(self):
        return self.__class__.__name__ + '(keys={}, order={})'.format(
            self.keys, self.order)


@PIPELINES.register_module
class ToDataContainer(object):

    def __init__(self,
                 fields=(dict(key='img', stack=True), dict(key='gt_bboxes'),
                         dict(key='gt_labels'))):
        self.fields = fields

    def __call__(self, results):
        for field in self.fields:
            field = field.copy()
            key = field.pop('key')
            results[key] = DC(results[key], **field)
        return results

    def __repr__(self):
        return self.__class__.__name__ + '(fields={})'.format(self.fields)


@PIPELINES.register_module
class DefaultFormatBundle(object):
    """Default formatting bundle.

    It simplifies the pipeline of formatting common fields, including "img",
    "proposals", "gt_bboxes", "gt_labels", "gt_masks" and "gt_semantic_seg".
    These fields are formatted as follows.

    - img: (1)transpose, (2)to tensor, (3)to DataContainer (stack=True)
    - proposals: (1)to tensor, (2)to DataContainer
    - gt_bboxes: (1)to tensor, (2)to DataContainer
    - gt_bboxes_ignore: (1)to tensor, (2)to DataContainer
    - gt_labels: (1)to tensor, (2)to DataContainer
    - gt_masks: (1)to tensor, (2)to DataContainer (cpu_only=True)
    - gt_semantic_seg: (1)unsqueeze dim-0 (2)to tensor,
                       (3)to DataContainer (stack=True)
    """

    def __call__(self, results):
        if 'img' in results:
            img = np.ascontiguousarray(results['img'].transpose(2, 0, 1))
            results['img'] = DC(to_tensor(img), stack=True)
        for key in ['proposals', 'gt_bboxes', 'gt_bboxes_ignore', 'gt_labels']:
            if key not in results:
                continue
            results[key] = DC(to_tensor(results[key]))
        if 'gt_masks' in results:
            results['gt_masks'] = DC(results['gt_masks'], cpu_only=True)
        if 'gt_semantic_seg' in results:
            results['gt_semantic_seg'] = DC(
                to_tensor(results['gt_semantic_seg'][None, ...]), stack=True)
        return results

    def __repr__(self):
        return self.__class__.__name__


@PIPELINES.register_module
class Collect(object):

    def __init__(self,
                 keys,
                 # meta_keys=('filename', 'ori_shape', 'img_shape', 'pad_shape',
                 #            'scale_factor', 'flip', 'img_norm_cfg')):
                 meta_keys=('ori_shape', 'img_shape', 'pad_shape', 'scale_factor', 'flip', 'img_norm_cfg')):
        self.keys = keys
        self.meta_keys = meta_keys

    def __call__(self, results):
        data = {}
        img_meta = {}
        for key in self.meta_keys:
            img_meta[key] = results[key]
        data['img_meta'] = DC(img_meta, cpu_only=True)
        for key in self.keys:
            data[key] = results[key]
        return data

    def __repr__(self):
        return self.__class__.__name__ + '(keys={}, meta_keys={})'.format(
            self.keys, self.meta_keys)
```

# 基于paddlepaddle模型库
## faster-rcnn实现

[项目地址](https://aistudio.baidu.com/aistudio/projectdetail/111503)

直接用模型库中的设置，除了降低学习率防止nan报错外完全没有调参，训练时间在2天左右。

比较哭笑不得的是该模型是paddle中提交的最好结果，其后针对faster-rcnn的调参效果非常不好，至于cascade-rcnn的模型则没有在提交前100%完成。

> 这里有个问题，无论是mmdetection和paddle的模型库都是把背景类也作为一个类别算进去的，不过在该项目中，没有把背景类加进去；而加了背景类的cascade-rcnn评估指标极差，不知道哪里出了问题（也可能是文件路径等问题）

### 计算像素均值

```python
# 计算像素均值
import os
import cv2
from numpy import *

img_dir='./coco/train2017'
img_list=os.listdir(img_dir)
img_size=224
sum_r=0
sum_g=0
sum_b=0
count=0

for img_name in img_list:
    img_path=os.path.join(img_dir,img_name)
    img=cv2.imread(img_path)
    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    img=cv2.resize(img,(img_size,img_size))
    sum_r=sum_r+img[:,:,0].mean()
    sum_g=sum_g+img[:,:,1].mean()
    sum_b=sum_b+img[:,:,2].mean()
    count=count+1

sum_r=sum_r/count
sum_g=sum_g/count
sum_b=sum_b/count
img_mean=[sum_r,sum_g,sum_b]
print (img_mean)
```

### 复现过程

- models/PaddleCV/rcnn目录下主要修改的文件

  - utility.py
  - config.py
  - eval_helper.py

- models/PaddleCV/rcnn/utility.py主要修改内容

  - 根据实际类别数修改class_num
  - 根据实际图片大小修改max_size和scales
  - 根据计算的像素均值修改pixel_means
  - 降低初始学习率learning_rate（否则会报错）
  - 修改data_dir和image_path等目录路径（也可以运行时再赋值）

  ```python
  def parse_args():
   """return all args
   """
   parser = argparse.ArgumentParser(description=__doc__)
   add_arg = functools.partial(add_arguments, argparser=parser)
   # yapf: disable
   # ENV
   add_arg('parallel',         bool,   True,       "Whether use parallel.")
   add_arg('use_gpu',          bool,  True,      "Whether use GPU.")
   add_arg('model_save_dir',   str,    'output',     "The path to save model.")
   add_arg('pretrained_model', str,    'imagenet_resnet50_fusebn', "The init model path.")
   add_arg('dataset',          str,   'coco2017',  "coco2014, coco2017.")
   # add_arg('class_num',        int,   81,          "Class number.")
   add_arg('class_num',        int,   20,          "Class number.")
   add_arg('data_dir',         str,   'dataset/coco',        "The data root path.")
   add_arg('use_pyreader',     bool,   True,           "Use pyreader.")
   add_arg('use_profile',         bool,   False,       "Whether use profiler.")
   add_arg('padding_minibatch',bool,   False,
       "If False, only resize image and not pad, image shape is different between"
       " GPUs in one mini-batch. If True, image shape is the same in one mini-batch.")
   #SOLVER
   add_arg('learning_rate',    float,  0.005,     "Learning rate.")
   add_arg('max_iter',         int,    180000,   "Iter number.")
   add_arg('log_window',       int,    20,        "Log smooth window, set 1 for debug, set 20 for train.")
   # RCNN
   # RPN
   add_arg('anchor_sizes',     int,    [32,64,128,256,512],  "The size of anchors.")
   add_arg('aspect_ratios',    float,  [0.5,1.0,2.0],    "The ratio of anchors.")
   add_arg('variance',         float,  [1.,1.,1.,1.],    "The variance of anchors.")
   add_arg('rpn_stride',       float,  [16.,16.],    "Stride of the feature map that RPN is attached.")
   add_arg('rpn_nms_thresh',    float,   0.7,          "NMS threshold used on RPN proposals")
   # TRAIN VAL INFER
   add_arg('MASK_ON', bool, False, "Option for different models. If False, choose faster_rcnn. If True, choose mask_rcnn")
   add_arg('im_per_batch',       int,   1,        "Minibatch size.")
   # add_arg('max_size',         int,   1333,    "The resized image height.")
   add_arg('max_size',         int,   2446,    "The resized image height.")
   # add_arg('scales', int,  [800],    "The resized image height.")
   add_arg('scales', int,  [1000],    "The resized image height.")
   add_arg('batch_size_per_im',int,    512,    "fast rcnn head batch size")
   # add_arg('pixel_means',     float,   [102.9801, 115.9465, 122.7717], "pixel mean")
   add_arg('pixel_means',     float,   [90.06717612279803, 88.77929274966118, 95.75007636987263], "pixel mean")
   add_arg('nms_thresh',    float, 0.5,    "NMS threshold.")
   add_arg('score_thresh',    float, 0.05,    "score threshold for NMS.")
   add_arg('snapshot_stride',  int,    10000,    "save model every snapshot stride.")
   # SINGLE EVAL AND DRAW
   add_arg('draw_threshold',  float, 0.8,    "Confidence threshold to draw bbox.")
   # add_arg('image_path',       str,   'dataset/coco/val2017',  "The image path used to inference and visualize.")
   add_arg('image_path',       str,   './coco/val2017',  "The image path used to inference and visualize.")
   # ce
   parser.add_argument(
           '--enable_ce', action='store_true', help='If set, run the task with continuous evaluation logs.')
   # yapf: enable
   args = parser.parse_args()
   file_name = sys.argv[0]
   if 'train' in file_name or 'profile' in file_name:
       merge_cfg_from_args(args, 'train')
   else:
       merge_cfg_from_args(args, 'val')
   return args
  ```

- models/PaddleCV/rcnn/config.py主要修改内容

   

  **（C.TRAIN和C.TEST都要改）**

  - 根据实际类别数修改class_num
  - 根据实际图片大小修改max_size和scales
  - 根据计算的像素均值修改pixel_means

- models/PaddleCV/rcnn/eval_helper.py修改内容

  - labels_map,直接改为中文会出现报错，因此标注暂时使用ID

  ```python
  def coco17_labels():
  labels_map = {
      1: '1',
      2: '2',
      3: '3',
      4: '4',
      5: '5',
      6: '6',
      7: '7',
      8: '8',
      9: '9',
      10: '10',
      11: '11',
      12: '12',
      13: '13',
      14: '14',
      15: '15',
      16: '16',
      17: '17',
      18: '18',
      19: '19',
      20: '20'
  }
  return labels_map
  ```



### 模型训练

```python
!python models/PaddleCV/rcnn/train.py --model_save_dir='./coco/output/' \
    --data_dir='./coco/' \
    --pretrained_model='models/PaddleCV/rcnn/pretrained/imagenet_resnet50_fusebn' \
    --MASK_ON=False
```

### 模型评估

```python
# 环境变量设置，可能无效
!export FLAGS_fraction_of_gpu_memory_to_use=0.92
!export FLAGS_initial_gpu_memory_in_mb=4096
!export FLAGS_reallocate_gpu_memory_in_mb=4096
```

```python
# 环境变量设置无效的情况下，需要重新在python中定义并运行一遍
def set_paddle_flags(flags):
    for key, value in flags.items():
        if os.environ.get(key, None) is None:
            os.environ[key] = str(value)
            
set_paddle_flags({
    'FLAGS_conv_workspace_size_limit': 500,
    'FLAGS_eager_delete_tensor_gb': 0,  # enable gc
    'FLAGS_memory_fraction_of_eager_deletion': 1,
    'FLAGS_fraction_of_gpu_memory_to_use': 0.98,
    'FLAGS_initial_gpu_memory_in_mb': 4096,
    'FLAGS_reallocate_gpu_memory_in_mb': 4096
})
```

```python
!python -u models/PaddleCV/rcnn/eval_coco_map.py \
    --pretrained_model='./coco/output/model_iter179999' \
    --data_dir='./coco/'  \
    --MASK_ON=False
```

### 模型推断及可视化

```python
!python -u models/PaddleCV/rcnn/infer.py \
   --pretrained_model='coco/output/model_iter179999' \
   --image_path='data/guangdong1_round1_testA_20190818/03bdb2d5d2f2183d0919134502.jpg' \
   --draw_threshold=0.3 \
   --MASK_ON=False
```

## PaddleDetection模型库cascade_rcnn实现

[项目地址](https://aistudio.baidu.com/aistudio/projectdetail/113365)

PaddleDetection和mmdetection类似，都是很完整的模型库，不过似乎没有公开太多预训练权重，这就需要重新训练了

```python
# 拉取模型库或直接打包PaddleDetection部分上传
# !git clone https://github.com/PaddlePaddle/models.git
Cloning into 'models'...
remote: Enumerating objects: 24, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 25631 (delta 5), reused 9 (delta 2), pack-reused 25607
Receiving objects: 100% (25631/25631), 257.06 MiB | 4.10 MiB/s, done.
Resolving deltas: 100% (16974/16974), done.
Checking connectivity... done.
```

```python
!pip install pycocotools
Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/
Requirement already satisfied: pycocotools in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (2.0.0)
```

```python
# 安装cocoAPI
# !git clone https://github.com/cocodataset/cocoapi.git
#if cython is not installed
#Install into global site-packages
!cd cocoapi/PythonAPI && make install
# Alternatively, if you do not have permissions or prefer
# not to install the COCO API into global site-packages
# !python cocoapi/PythonAPI/setup.py install --user
```

```python
!pip install tqdm
```

```
# 源代码似乎有点bug，要修改路径
!cp PaddleDetection/tools/* PaddleDetection/
```



### 一些主要设置调整

- PaddleDetection/ppdet/data/data_feed.py调整image_size
- PaddleDetection/ppdet/modeling/roi_heads/cascade_head.py调整num_classes（也可以统一使用 **shared** = ['num_classes']）
- PaddleDetection/ppdet/modeling/ops.py调整num_classes
- PaddleDetection/ppdet/modeling/target_assigners.py调整num_classes



### cascade_rcnn_dcn_r50_fpn_1x参考配置文件

PaddleDetection/configs/dcn/cascade_rcnn_dcn_r50_fpn_1x.yml

调参参考 [布匹疵点智能识别方案介绍及baseline(acc:85%左右,mAP:52%左右)](https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.3.43b464487UpLDj&postId=74264)

**设置类别时没有把背景类算入**

```yaml
architecture: CascadeRCNN
train_feed: FasterRCNNTrainFeed
eval_feed: FasterRCNNEvalFeed
test_feed: FasterRCNNTestFeed
max_iters: 720000
snapshot_iter: 80000
use_gpu: true
log_smooth_window: 20
log_iter: 20
save_dir: output
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/ResNet50_cos_pretrained.tar
weights: output/cascade_rcnn_dcn_r50_fpn_1x/model_final
metric: COCO
num_classes: 20

CascadeRCNN:
  backbone: ResNet
  fpn: FPN
  rpn_head: FPNRPNHead
  roi_extractor: FPNRoIAlign
  bbox_head: CascadeBBoxHead
  bbox_assigner: CascadeBBoxAssigner

ResNet:
  norm_type: bn
  depth: 50
  feature_maps: [2, 3, 4, 5]
  freeze_at: 2
  variant: b
  dcn_v2_stages: [3, 4, 5]

FPN:
  min_level: 2
  max_level: 6
  num_chan: 256
  spatial_scale: [0.03125, 0.0625, 0.125, 0.25]

FPNRPNHead:
  anchor_generator:
    anchor_sizes: [32, 64, 128, 256, 512]
    aspect_ratios: [0.02, 0.05, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0]
    stride: [4, 8, 16, 32, 64]
    variance: [1.0, 1.0, 1.0, 1.0]
  anchor_start_size: 32
  min_level: 2
  max_level: 6
  num_chan: 256
  rpn_target_assign:
    rpn_batch_size_per_im: 256
    rpn_fg_fraction: 0.5
    rpn_positive_overlap: 0.7
    rpn_negative_overlap: 0.3
    rpn_straddle_thresh: 0.0
  train_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 2000
    post_nms_top_n: 2000
  test_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 1000
    post_nms_top_n: 1000

FPNRoIAlign:
  canconical_level: 4
  canonical_size: 224
  min_level: 2
  max_level: 5
  box_resolution: 7
  sampling_ratio: 2

CascadeBBoxAssigner:
  batch_size_per_im: 512
  bbox_reg_weights: [10, 20, 30]
  bg_thresh_lo: [0.0, 0.0, 0.0]
  bg_thresh_hi: [0.5, 0.6, 0.7]
  fg_thresh: [0.5, 0.6, 0.7]
  fg_fraction: 0.25

CascadeBBoxHead:
  head: CascadeTwoFCHead
  nms:
    keep_top_k: 100
    nms_threshold: 0.5
    score_threshold: 0.05

CascadeTwoFCHead:
  mlp_dim: 1024

LearningRate:
  base_lr: 0.00125
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [480000, 640000]
  - !LinearWarmup
    start_factor: 0.1
    steps: 1000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2

FasterRCNNTrainFeed:
  batch_size: 2
  dataset:
    dataset_dir: dataset/coco
    annotation: annotations/instances_train2017.json
    image_dir: train2017
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32
  drop_last: false
  num_workers: 2

FasterRCNNEvalFeed:
  batch_size: 1
  dataset:
    dataset_dir: dataset/coco
    annotation: annotations/instances_val2017.json
    image_dir: val2017
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32

FasterRCNNTestFeed:
  batch_size: 1
  dataset:
    annotation: dataset/coco/annotations/instances_val2017.json
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32
  drop_last: false
  num_workers: 2
```

### 模型训练

```bash
# cascade_rcnn_dcn_r50_fpn_1x训练
!cd PaddleDetection/ && python -u train.py -c configs/dcn/cascade_rcnn_dcn_r50_fpn_1x.yml --eval
```

### 生成预测图像

```bash
!python PaddleDetection/infer.py -c PaddleDetection/configs/dcn/cascade_rcnn_dcn_r50_fpn_1x.yml \
-o weights=PaddleDetection/output/cascade_rcnn_dcn_r50_fpn_1x/model_final \
--infer_dir=data/guangdong1_round1_testA_20190818/ \
--draw_threshold=0.5
```
